
\section{Backend-Independent Vectorization}
\label{sec:vectorization}

%\ana{Need an intro/overview here!}
%\ana{Need to define notion of canonical access!}
%\ana{Talk about MPC Source, opportunities for vectorization, algorithm unique to MPC Source.}

This section describes our vectorization algorithm. While vectorization is a longstanding problem, 
and we build upon existing work on scalar expansion and classical loop vectorization~\cite{Allen:1987}, 
our algorithm is unique as it works on the MPC Source SSA-form representation. We posit that vectorization 
over MPC Source is a new problem that warrants a new look, in part because of MPC's unique linear 
structure and in part because vectorization meshes in with other MPC-specific optimizations in non-trivial ways
(other works have explored manual vectorization and protocol mixing in an ad-hoc way, e.g.,~\cite{NDSS:DemSchZoh15,CCS:BDKKS18,Ishaq:2019}).

In this section, we briefly describe the analysis. Details, including edge cases and examples, can be found in the supplementary extended version~\cite{Anon_TR}.
\secref{sec:dependence} describes our dependence analysis and ~\secref{sec:expansion} describes scalar expansion,
which lifts scalars (and arrays) to the corresponding loop dimensionality to create opportunities for vectorization. 
\secref{sec:basic_vectorization} describes our core vectorization algorithm and~\secref{sec:correctness} argues correctness of the vectorization transformation.
\secref{sec:array_writes} extends vectorization with array writes.   

\subsection{Dependence Analysis}
\label{sec:dependence}

We build a dependence graph where the nodes are the MPC Source statements and the edges represent the def-use relations.

\paragraph{Def-use Edges} We distinguish the following def-use edges:

\begin{itemize}
\item same-level edge $X\rightarrow Y$ where $X$ and $Y$ are in the same loop nest, say $i,j,k$. E.g., the def-use edge 9 to 10 in the Biometric MPC Source in Listing~\ref{tab:source_and_MPC_source_and_optimized_MPC_source}(b) is a same-level edge. 
%A same-level edge can be a back-edge in which case a $\phi$ node is the target of the edge. E.g., 15 to 4 in Biometric is a same-level back-edge.
\item outer-to-inner $X\rightarrow Y$ where $X$ is in an outer loop nest, say $i$, and $Y$ is in an inner one, say $i,j,k$. E.g., 1 to 4 in Biometric forms is an outer-to-inner edge.
%A $\phi$-node can be a source or a target of an outer-to-inner forward edge.
\item inner-to-outer $X\rightarrow Y$ where $X$ is a $phi$-node in an inner loop nest, $i,j,k$, and $Y$ is in the enclosing loop nest $i,j$. E.g., the def-use from 8 to 12 gives rise to an inner-to-outer edge.
%E.g. $\texttt{sum}_0 = \phi(\texttt{sum}_1, 0)$ to $\texttt{c} = \mathit{CMP}(\texttt{sum}_0,\texttt{min}_0)$ is an inner-to-outer edge.
%An inner-to-outer edge can be a back-edge as well, in which case both $X$ and $Y$ are $\phi$-nodes with the source $X$ in a loop nested into $Y$'s loop (not necessarily immediately).

%Note that the source is \emph{always} a $\phi$-node of the $i,j,k$ loop and the target is in the immediately enclosing loop. The interpretation of this edge is that the use node $Y$ uses the definition made in the last iteration of the inner loop.
%\ben{The current representation of pseudo $\phi$-nodes shows them attached to the loop header (i.e. in the loop).  We may want to clarify that they are also evaluated at the loop's termination.}
%\item same-level back-edge $X\rightarrow Y$. $Y$ is a phi-node in the header of the loop and $X$ is a definition of the variable in the loop body. E.g., $\texttt{min}_1 = \mathit{MUX}(\texttt{c},\texttt{sum}_1,\texttt{min}_1)$ to $\texttt{min}_0 = \phi(\texttt{min}_1, 10000)$ in Biometric is a same-level back-edge.
%\item inner-to-outer back-edge $X\rightarrow Y$: $X$ and $Y$ are both $\phi$-nodes for some variable. The source $X$ is in a loop nested into $Y$'s loop (not necessarily immediately).

\item mixed forward edge $X\rightarrow Y$. $X$ is in some loop $i,j,k$ and $Y$ is in a loop nested into $i,j,k'$. We transform mixed forward edges as follows. Let $\x$ be the variable defined at $X$. We add a variable and assignment $\x' = \x$ immediately after the $i,j,k$ loop. Then we replace the use of $\x$ at $Y$ with $\x'$. This transforms a mixed forward edge into an "inner-to-outer" forward edge followed by an outer-to-inner forward edge. %Thus, Basic Vectorization handles one of "same-level", "inner-to-outer", or "outer-to-inner" def-use edges.
 \end{itemize}

\paragraph{Closures}
We define $\mathit{closure}(n)$ where $n$ is a PHI-node. Intuitively, it computes the set of nodes (i.e., statements) that form a dependence cycle with $n$. The closure of $n$ is defined as follows:
\begin{itemize}
\item $n$ is in $\mathit{closure}(n)$
\item $X$ is in $\mathit{closure}(n)$ if there is a same-level path from $n$ to $X$, and $X \rightarrow n$ is a same-level back-edge.
\item $Y$ is in $\mathit{closure}(n)$ if there is a same-level path from $n$ to $Y$ and there is a same-level path from $Y$ to some $X$ in $\mathit{closure}(n)$.
\end{itemize}

\subsection{Scalar Expansion}
\label{sec:expansion}

%\subsubsection{Overview}

An important component of our algorithm is expansion of scalars and arrays to the corresponding loop dimensionality, 
which is necessary to expose opportunities for vectorization.
In the Biometric example, {\sf d = S[i*D+j] - C[j]} equiv. to {\sf d = S[i,j] - C[j]}, which gave rise to $N*D$ subtraction operations in the sequential schedule,
is lifted. The argument arrays {\sf S} and {\sf C} are lifted and the scalar {\sf d} is lifted: {\sf d[i,j] = S[i,j] - C[i,j]}.
The algorithm then detects that the statement can be vectorized.

Expansion (and also reduction) is done with the $\mathit{raise\_dim}$ and $\mathit{drop\_dim}$ functions, 
which we believe are standard. $\mathit{raise\_dim}$ takes the original array, the access function 
$f(i,j,k)$ in loop nest $i,j,k$ and the loop bounds $((i\!:\!I),(j\!:\!J),(k\!:\!K))$ and produces a new 3-dimensional array 
${\sf A'}$ by iterating over $i,j,k$ and setting each element of  ${\sf A'}$:
\[ \mathit{raise\_dim}(A, f(i,j,k), ((i\!:\!I),(j\!:\!J),(k\!:\!K))): {\sf A'}[i,j,k] =  {\sf A}[f(i,j,k)] \]
Raise dimension applies when reshaping input arrays and also, at outer-to-inner def-use edges.
Analogously $\mathit{drop\_dim}$ applies at inner-to-outer def-use edges.
It takes a higher dimensional array, say $i,j,k$ and removes trailing dimensions, say $j,k$. 
It iterates over $i$ and takes the result at the maximal index of $j$ and $k$, i.e., the result at the last iterations of $j$ and $k$:
\[ \mathit{drop\_dim}({\sf A}, (j\!:\!J,k\!:\!K)): {\sf A'}[i] = A[i,J-1,K-1] \] 
%It iterates over $i$ and takes the result at the maximal index of $j$ and $k$, i.e., the result at the last iterations of $j$ and $k$:
%\[ {\sf A'}[i] = A[i,J-1,K-1] \]


\begin{comment}
\paragraph{Arrays} Conceptually, we treat all variables as arrays. There are three kinds of arrays. 
%\ana{Arrays is not a good name here.}

\begin{itemize}
\item Scalars: These are scalar variables we expand into arrays for the purposes of vectorization.
For those, all writes are canonical writes and all reads are canonical reads. We will \emph{raise dimension} when
a scalar gives rise to an outer-to-inner dependence edge (e.g., {\sf sum!2} in line 6 of the MPC source code will be raised to a 1-dimensional
array since {\sf sum!2} is used in the inner $j$-loop). We will \emph{drop dimension} when a scalar
gives rise to an inner-to-outer dependence edge (e.g., {\sf sum!2} for which the lifted inner loop computes
{\sf D} values, but the outer loop only needs the last one.)

\item Read-only input arrays: There are no writes, while we may have non-canonical reads, $f(i,j,k)$.
Vectorization adds raise dimension operations at the beginning of the function to lift these arrays to 
the dimensionality of the loop where they are used, possibly \emph{reshaping} the arrays. 
If there are multiple ``views'' of the input array, there would be multiple raise dimension statements to create
each one of these views. The invariant is that at reads in loops, the reads of "views" of the original input array are canonical.
%Only raise dimension applies to these arrays, and only in the beginning of the program. For example, 1-dimensional array
%\python{C} is lifted into 2-dimensional array {\sf N x D} by copying the row {\sf N} times.

 \item Read-write output arrays: Writes are canonical (by restriction) but reads can be non-canonical.
 Dependences limit vectorization when non-canonical read access refers to array writes in
 previous iterations, thus creating loop-carried dependences. We may apply both raise and drop dimension, however,
 they respect the fixed dimensionality of the output array. The array cannot be raised to a dimension lower than
 its canonical (fixed) dimensionality and it cannot be dropped to lower dimension. In addition, non-canonical reads may
 require lifting (i.e., reshaping) of the array after the most recent write rather than in the beginning of the program
 in order to reduce a non-canonical read to a canonical one.

\end{itemize}

%In the sections below we detail the $\mathit{raise\_dim}$ (raise dimension) and $\mathit{drop\_dim}$ (drop dimension) operations,
%followed by our vectorization algorithm.

%\subsubsection{Raise Dimension and Drop Dimension}

\paragraph{Raise dimension} 
The $\mathit{raise\_dim}$ function expands a scalar (or array). 
There are two conceptual versions of $\mathit{raise\_dim}$. One reshapes an arbitrary access into a canonical read access in the corresponding loop. 
%The signature of $\mathit{raise\_dim}$ is as follows. 
It takes the original array, the access pattern function $f(i,j,k)$ in loop nest $i,j,k$ and the loop bounds
$((i:I),(j:J),(k:K))$:
\[ \mathit{raise\_dim}(A, f(i,j,k), ((i:I),(j:J),(k:K))) \]
It produces a new 3-dimensional array ${\sf A'}$ by iterating over $i,j,k$ and setting each element of  ${\sf A'}$ as follows:
\[ {\sf A'}[i,j,k] =  {\sf A}[f(i,j,k)] \]
The end result is that uses of ${\sf A}[f(i,j,k)]$ in loop nest $i,j,k$ are replaced with canonical read-accesses to ${\sf A'}[i,j,k]$
that can be vectorized. In the running Biometric example, ${\sf C'} = \mathit{raise\_dim}({\sf C}, j, (i:N,j:D))$ lifts the
1-dimensional array {\sf C} into a 2-dimensional array. The $i,j$ loop now accesses ${\sf C'}$ in the canonical way, ${\sf C'}[i,j]$.
Similarly, ${\sf S'} = \mathit{raise\_dim}({\sf S}, i*D+j, (i:N,j:D))$ tries to lift {\sf S}, but the operation turns into a no-op
because {\sf S} is already a 2-dimensional array and the read access is canonical.

The other version of $\mathit{raise\_dim}$ lifts a lower-dimension array into a
higher-dimension for access in a nested loop. It is necessary when processing outer-to-inner dependences. 
Here {\sf A} is an $i$-array and raise dimension adds two additional dimensions:
\[ \mathit{raise\_dim}({\sf A}, (j:J,k:K)) \]
This version is reduced to the above version by adding the access pattern function, which is just $i$:
\[ \mathit{raise\_dim}({\sf A}, i, (j:J,k:K)) \]

\paragraph{Drop dimension}  
The corresponding $\mathit{drop\_dim}$ is carried out when an array written in an inner loop is used in an enclosing loop.
It takes a higher dimensional array, say $i,j,k$ and removes trailing dimensions, say $j,k$:
\[ \mathit{drop\_dim}({\sf A}, (j:J,k:K)) \]
It iterates over $i$ and takes the result at the maximal index of $j$ and $k$, i.e., the result at the last iterations of $j$ and $k$:
\[ {\sf A'}[i] = A[i,J-1,K-1] \]
\end{comment}

\subsection{Basic Vectorization}
\label{sec:basic_vectorization}

%\subsubsection{Algorithm}

There are two key phases of the algorithm. Phase 1 inserts raise dimension and drop dimension 
operations according to def-uses. E.g., if there is an inner-to-outer dependence, it inserts $\mathit{raise\_dim}$,
and similarly, if there is an outer-to-inner dependence, it inserts $\mathit{drop\_dim}$. After this phase operations work
on arrays of the corresponding dimensionality and we optimistically vectorize all arrays.

Phase 2 proceeds from the inner-most towards the outer-most loop. For each loop it anchors dependence cycles (closures) 
around pseudo PHI nodes then removes vectorization from the dimension of that loop. There are two important points in this 
phase. First, it may break a loop into smaller loops which would discover opportunities 
for vectorization in intermediate statements in the loop. Second, it handles writes arrays. 
It creates opportunities for vectorization in the presence of write arrays, even though 
Cytron's SSA adds a back-edge to the array PHI-node, thus killing vectorization of statements
that read and write that array.

The excerpts in {\color{red} red} color in the pseudo code below highlight the extension 
with array writes. We advise the reader to omit the extension for now and consider just read-only
arrays. We explain the extension in \secref{sec:array_writes}. 
(As many of our benchmarks include write arrays, it plays an important role.)

%The pseudo-code in red deals with this extension
%of the basic algorithm. We determine that some array pseudo-PHIs are \emph{target-less}, meaning
%that there is no loop-carried dependence for that array and back-edges into these nodes can be safely removed. 
%This opens up opportunities for vectorization across the write array. Handling of target-less nodes is non-trivial, 
%as in certain cases the node cannot be removed. \secref{sec:extension_with_writes} details our algorithm. 

Phases 3 cleans up local arrays of references (this is an optional phase and our current implementation does not include it)
and Phase 4 explicitly turns operations into MOTION SIMD operations.

\paragraph{Pseudocode:}

\begin{algorithmic}

\STATE \COMMENT { Phase 1: Raise dimension of scalar variables to corresponding loop nest. We can traverse stmts linearly in MPC-source. }

\FOR {each MPC $\mathit{stmt}: \x = \mathit{Op}(\y_1,\y_2)$ in loop $i,j,k$}
%\FOR {each $Y_n$}
\FOR {each argument $\y_n$} %def-use edge $\mathit{stmt}' (\mbox{def of } Y) \rightarrow \mathit{stmt} (\mbox{def of } X)$}
\STATE {case def-use edge $\mathit{stmt}' (\mbox{def of } \y_n) \rightarrow \mathit{stmt} (\mbox{def of } \x)$ of} \\
\STATE \hspace{0.25cm} {same-level: $\y'_n$ is $\y_n$} \\
\STATE \hspace{0.25cm} {outer-to-inner: add $\y'_n[i,j,k] = \mathit{raise\_dim}(\y_n)$ at $\mathit{stmt'}$}
\STATE \hspace{0.25cm} {(more precisely, right after $\mathit{stmt'}$)}
\STATE \hspace{0.25cm} {inner-to-outer: add $\y'_n[i,j,k] = \mathit{drop\_dim}(\y_n)$ at $\mathit{stmt}$}
\STATE \hspace{0.25cm} {(more precisely, in loop of $\mathit{stmt}$ right after loop of $\mathit{stmt'}$)}
\ENDFOR

\COMMENT { Optimistically vectorize all. $I$ means vectorized dimension. }
%\FOR {each MPC $\mathit{stmt}: X = \mathit{Op}(Y_1,Y_2)$ in loop $i,j,k$}
\STATE {change to $\x[I,J,K] = \mathit{Op}(\y'_1[I,J,K],\y'_2[I,J,K])$ }
\ENDFOR

\COMMENT { Phase 2: Recreating for-loops for cycles; vectorizable stmts hoisted up. }

\FOR {each dimension $d$ from highest to 0}
\FOR {each PHI-node $n$ in loop $i_1,...,i_d$}
\STATE {compute $\mathit{closure}(n)$}
\ENDFOR

\COMMENT {{\color{red} $cl_1$ and $cl_2$ intersect if they have common statement or update same array; "intersect" definition can be expanded}}

\WHILE {there are closure $cl_1$ and $cl_2$ that intersect}
\STATE {merge $cl_1$ and $cl_2$}
\ENDWHILE
\FOR {each closure $cl$ (after merge)}
\STATE {create \python{for} $i_d$ \python{in} ... loop} \COMMENT{ i.e., MOTION loop }
\STATE {add PHI-nodes in $cl$ to header block} 
\STATE {{\color{red}{add target-less PHI-node for A if $cl$ updates array A}}}
\STATE {add statements in $cl$ to loop in some order of dependences}
\STATE \COMMENT { Dimension is not vectorizable: }
\STATE {change ${I_d}$ to $i_d$ in all statements in loop}
\STATE {treat \python{for}-loop as monolith node for def-uses: e.g., some def-use edges become same-level.}
\ENDFOR
{
\color{red}
\FOR {each target-less PHI-node {\sf A!1 = PHI(A!0,A!k)}}
\STATE {in vectorizable stmts, replace use of {\sf A!1} with {\sf A!0}}
\STATE {discard PHI-node if not used in any $cl$, replacing {\sf A!1} with {\sf A!0} or {\sf A!k} appropriately}
\ENDFOR
}
\ENDFOR

\COMMENT { Phase 3: Remove unnecessary dimensionality. }

\COMMENT { A dimension $i$ is dead on exit from stmt $\x[...i...] = ...$ if all def-uses with targets outside of the enclosing
\python{for} $i$ ... MOTION loop end at target (use) $\x' = \mathit{drop\_dim}(\x, i)$. }
\FOR {each stmt and dimension $\x[...i...] = ...$}
\STATE {if $i$ is a dead dimension on exit from stmt $\x[...i...] = ...$, remove $i$ from $\x$ (all defs and uses)}
%\COMMENT{ A stmt v[...i] is a "dead dimension i" if all edges out of FOR i = 0; ... loop end at drop\_dim(v, i)}
\ENDFOR

\COMMENT { Now clean up drop\_dim and raise\_dim }
\FOR {each $\x' =  \mathit{drop\_dim(\x,i)}$ }
\STATE {replace with $\x' = \x$ if $i$ is dead in $\x$.}
\ENDFOR
\STATE {do (1) (extended) constant propagation, (2) copy propagation and (3) dead code elimination to get rid of redundant variables and raise and drop dimension statements}

\COMMENT { Phase 4: }
\STATE {add SIMD for simdified dimensions}

\end{algorithmic}

\input{biometric_vectorization_example.tex}

\subsection{Correctness Argument}
\label{sec:correctness}

We build a correctness argument that loosely follows Abstract Interpretation.
First we define the MPC Source syntax. The domain of MPC Source programs
expressible in the syntax (with certain semantic restrictions) is the abstract domain $A$.
We then define the \emph{linearization} of an MPC Source program as an interpretation
over the syntax. The linearization, which is a \emph{schedule} (as in~\secref{sec:overview}), is the concrete domain $C$.
Since we reason over def-use graphs in $A$ we define a partial order relation over elements of $A$
in terms of def-use relations. We define a partial order over elements of $C$ as well,
in terms of def-use relations in the concrete domain $C$. We prove two theorems
that state (informally) that the schedule corresponding to the original program computes the same
result as the schedule corresponding to the vectorized program. 

\begin{comment}
\begin{figure}%[!b]
%\small
\[
\begin{array}{l@{~~~~~}l}
  \begin{array}{l@{~}l@{~~~}l}
  s &::= s;s & sequence \\
     & \mid \x[i,J,k] = \y[i,J,k] \; \mathit{op\_SIMD} \; \z[i,J,k] & operation \\
     & \mid \x[i,J,k]  = \texttt{PHI}(\x_1[i,J,k],\x_2[i,J,k\!-\!1]) & \mathit{phi} \; \mathit{node} \\
     & \mid \x[i,J,k] = \mathit{raise\_dim}(\x'[i],(J\!:\!J,k\!:\!K)) & \mathit{raise} \; \mathit{dimension(s)} \\
     & \mid \x[i,J] = \mathit{drop\_dim}(\x'[i,J,k],k)  & \mathit{drop} \; \mathit{dimension(s)} \\
%     & \mid \x = \y & \mathit{propagation} \\
     & \mid \texttt{FOR} \; 0 \le i < I: s  & \mathit{loop} \\
  \end{array}    \\
\end{array}
\]
\vspace{-3mm}
\caption{MPC Source Syntax. \ana{Add more to caption.}}\label{fig:MPC_Source_syntax}
\vspace{-5mm}
\end{figure}
\end{comment}

\begin{figure*}%[!b]
\[
\begin{array}{l@{~~~~~}l}
  \begin{array}{l@{~~~~~~~~~}l@{~~~~~~~~~}l@{~~~~~~~~~}l}
  s &::= s_1;s_2 & \gamma(s) = \gamma(s_1) \; ; \; \gamma(s_2) & \mathit{sequence} \\
     & \mid \x[i,J,k] = {\sf op\_SIMD}(\y_1[i,J,k], \y_2[i,J,k]) & \gamma(\x[i,J,k] = {\sf op\_SIMD}(\y_1[i,J,k], \y_2[i,J,k])) = & \mathit{operation} \\ 
     &                                                                               & ~~\x[i,0,k] = \y_1[i,0,k] \; {\sf op} \; \y_2[i,0,k] \;\; || &  \\
     &                                                                               & ~~~~\x[i,1,k] = \y_1[i,1,k] \; {\sf op} \; \y_2[i,1,k] \;\; || \; ...\; || & \\
     &                                                                               & ~~~~~~\x[i,J\!-\!1,k] = \y_1[i,J\!-\!1,k] \; {\sf op} \; \y_2[i,J\!-\!1,k] & \\
     & \mid \x[i,J,k] = {\sf const} & \mbox{analogous} & \mathit{constant} \\
     & \mid \x[i,J,k]  = {\sf PHI}(\x_1[i,J,k],\x_2[i,J,k\!-\!1]) & & \mathit{pseudo} \; \mathit{PHI} \\
     & \mid \x[i,J,k] = {\sf raise\_dim}(\x'[i],(J\!:\!J,k\!:\!K)) & & \mathit{raise} \; \mathit{dimension(s)} \\
     & \mid \x[i,J] = {\sf drop\_dim}(\x'[i,J,k],k) &  & \mathit{drop} \; \mathit{dimension(s)} \\
%     & \mid \x = \y & & \mathit{propagation} \\
     & \mid {\sf for} \; i \; {\sf in} \; {\sf range}(I): s  & \gamma({\sf for} \; i \; {\sf in} \; {\sf range}(I): s) = & \mathit{loop} \\
     & & ~~\gamma(s)[0/i] \; ; \; \gamma(s)[1/i] \; ; \; ... \; ; \; \gamma(s)[I\!-\!1/i] & \\
  \end{array}    \\
\end{array}
\]
\vspace{-3mm}
\caption{MPC Source Syntax and Semantics. $\gamma$ defines the semantics of MPC source which is a linearization of MPC Source. A SIMD operation parallelizes operations across the vectorized $J$ dimension. $||$ denotes parallel execution, which is standard. $\gamma$ of a for loop unrolls the loop. $;$ denotes sequential execution. Iterative MPC Source trivially extends to non-vectorized dimensions over the enclosing loops.}\label{fig:MPC_Source_syntax}
\label{fig:mpc_syntax}
\vspace{-5mm}
\end{figure*}

\paragraph{MPC Source Syntax}

\figref{fig:mpc_syntax} states the syntax and linearization semantics of MPC Source. 
Although notation is heavy, the linearization simply produces schedules as discussed in~\secref{sec:model}; 
the iterative MPC Source gives rise to what we called sequential schedule where loops are unrolled 
and MPC Source with vectorized dimensions gives rise to what we called parallel schedule. 
For simplicity, we consider only scalars and read-only arrays, however, the treatment extends to write arrays as well.
$\x[i,J,k]$ denotes the value of scalar variable \x{} at loop nest $i,j,k$. 
%(We use 3 dimensions, $i,j,k$, however, the treatment generalizes to arbitrary dimensions.) 
Upper case $J$ denotes a vectorized dimension and lower case $i,k$ denote iterative dimensions. Our compiler imposes 
semantic restrictions over the syntax: (1) $\x$ is treated as a 3-dimensional array and (2) $\x[i,J,k]$ must be enclosed
into for-loops on non-vectorized dimensions $i$ and $k$:
\begin{pythonn}
for i in range(I):
   ...
   for k in range(K):
      ... x[i,J,k] ...
\end{pythonn}


\begin{comment}
\figref{fig:MPC_Source_syntax} defines the syntax for our intermediate representation, MPC Source.
Note that this is a generalization of MPC Source to include vectorized MPC Source.
There are semantic restrictions over the syntax as well: a variable $\x[i,j,k]$ is a 3-dimensional array
$(i:I,j:J,k:K)$ and also, a statement $\x[i,J,k] = ...$ is enclosed in loops over the non-vectorized dimensions 
$i$ and $k$ as shown below. Thus, $i$ and $k$ are in scope.
\begin{pythonn}
for i in range(I):
    ...
    for k in range(K):
        x[i,J,k] = ...
\end{pythonn}

%Statements $\mathit{operation, phi, raise \; dimension(s), drop \; dimension(s)}$ are \emph{base} statements,
%and $\mathit{sequence, loop}$ are \emph{compound statements}.

\paragraph{Linearization}

Linearization is the concretization operation, which, as we mentioned earlier computes a schedule.
The concretization function $\gamma: A \rightarrow C$ is defined as an interpretation of
MPC Source syntax, as it is standard. The concretization of each one
of the base statements is as follows:
\[\begin{array}{l}
\gamma(\x[i,J,k] = \mathit{op\_SIMD}(\y[i,J,k], \z[i,J,k])) = \\
~~\x[i,0,k] = \y[i,0,k] \; \mathit{op} \; \z[i,0,k] \;\; || \\
~~~~\x[i,1,k] = \y[i,1,k] \; \mathit{op} \; \z[i,1,k] \;\; || \; ...\; || \\
~~~~~~\x[i,I-1,k] = \y[i,I-1,k] \; \mathit{op} \; \z[i,I-1,k]
\end{array}\]
meaning that the vectorized dimension(s) are expanded into parallel statements. || introduces SIMD (parallel) execution.

The concretization of the FOR statement is as follows:
\[ \gamma(\texttt{FOR} \; 0 \le i < I: s) = \gamma(s)[0/i] \; ; \; \gamma(s)[1/i] \; ; \; ... \gamma(s)[I-1/i] \]
$\gamma$ simply unrolls the loop substituting $i$ with 0, 1, etc. Here ; denotes sequential execution.
\end{comment}

\begin{comment}
\paragraph{Partial Orders}

For each MPC Source program $a$ we compute the def-use edges in the standard way: if base statement $s1 \in a$ defines variable \x, e.g., $\x[i,j,k] = ...$,
and base statement $s2 \in a$ uses \x, e.g., $... = ... \x[i,j,k]$ and there is a path in the trivial CFG from $s1$ to $s2$, then there is a def-use edge from
 $s1$ to $s2$. We extend the dimensionality of a statement into $s1[i,j,k]$ where $s1[i,j,k]$ inherits the dimensionality of the left-hand-side of the assignment.

 Let $a_0, a_1$ be two MPC Source programs in $A$.
 Two base statements, $s_0 \in a_0$ and $s_1 \in a_1$ are \emph{same}, written $s_0 \equiv s_1$ if they are of the same operation and they operate on
 the same variables: same variable name and same dimensionality. Recall that dimensions in MPC Source are either iterative, lower case, or vectorized, upper case.
 Two statements are same even if one operates on an iterative dimension and the other one operates on a vectorized one, e.g., $s_0[i,j,k] \equiv s_1[I,j,K]$.

 \begin{definition} Let $a_0, a_1 \in A$. We say that $a_0 \le a_1$ iff for every def-use edge $s1 \rightarrow s2$ in $a_0$ there is an edge
 $s1' \rightarrow s2'$ where $s1 \equiv s1'$, $s2 \equiv s2'$ and the two edges of either both forward or both backward.
 \end{definition}

 The def-use edges in the concrete schedule are as expected. There is a def use edge from statement $s1$ that defines $\x[\underline{i},\underline{j},\underline{k}]$
 to statement $s2$ that uses $\x[\underline{i},\underline{j},\underline{k}]$ if $s1$ is scheduled ahead of $s2$ in the linear schedule. We note that the underlined
 indices, e.g., $\underline{i}$, refer to fixed values, not iterative or vectorized dimensions since in the concrete schedule all induction variables are expanded.
 E.g., there is a def-use edge from the statement that defines $\x[0,1,2]$ and a statement that uses $\x[0,1,2]$.

\paragraph{Theorems}

\begin{theorem} $a_0 \le a_1 \Rightarrow \gamma(a_0) \subseteq \gamma(a_1)$.
\end{theorem}

\begin{theorem} Let $a_0$ be the iterative MPC Source and let $a_1$ be the vectorized MPC Source computed by the vectorization algorithm.
We have that $a_0 \le a_1$.
\end{theorem}

\ana{Write the proof sketch and final argument, etc.}
\end{comment}

\paragraph{Partial Orders}

For each MPC Source program $a$ we compute the def-use edges in the standard way: if statement $s1 \in a$ defines variable \x, e.g., $\x[i,j,k] = ...$, 
and statement $s2 \in a$ uses \x, e.g., $... = ... \x[i,j,k]$ and there is a path in CFG from $s1$ to $s2$, then there is a def-use edge from
 $s1$ to $s2$. We extend the dimensionality of a statement into $s1[i,j,k]$ where $s1[i,j,k]$ inherits the dimensionality of the left-hand-side of the assignment.
    
 Let $a_0, a_1$ be two MPC Source programs in $A$. 
 Two statements, $s \in a_0$ and $s' \in a_1$ are \emph{same}, written $s \equiv s'$ if they are of the same operation and they operate on 
 the same variables: same variable name and same dimensionality. Recall that dimensions in MPC Source are either iterative (lower case), or vectorized (upper case). 
 Two statements are same even if one operates on an iterative dimension and the other one operates on a vectorized one, e.g., $s[i,j,k] \equiv s'[I,j,K]$.
 We extend the definition to def-use edges in the obvious way: a def-use edge $s_0 \rightarrow s_1$ in $a_0$ and an edge $s'_0 \rightarrow s'_1$ in $a_1$
 are \emph{same}, written $s_0 \rightarrow s_1 \equiv s'_0 \rightarrow s'_1$, if and only if $s_0 \equiv s'_0$, $s_1 \equiv s'_1$, and the two edges are both either 
 forward or backward. 
  
\begin{definition} Let $a_0, a_1 \in A$. We say that $a_0 \le a_1$ iff for every def-use edge $e$ in $a_0$ there is an edge
$e'$ in $a_1$ such that $e \equiv e'$.
\end{definition}
 
 The def-use edges in the concrete schedule are as expected: there is a def-use edge from statement $s1$ that defines $\x[\underline{i},\underline{j},\underline{k}]$
 to statement $s2$ that uses $\x[\underline{i},\underline{j},\underline{k}]$ if $s1$ is scheduled ahead of $s2$ in the linear schedule. We note that the underlined
 indices, e.g., $\underline{i}$, refer to fixed values, not iterative or vectorized dimensions since in the concrete schedule all induction variables are expanded. 
 E.g., there is a def-use edge from the statement that defines $\x[0,1,2]$ and a statement that uses $\x[0,1,2]$.
 
\paragraph{Theorems} The two theorems arising from the Basic vectorization optimization are as follows: 

\begin{theorem} $a_0 \le a_1 \Rightarrow \gamma(a_0) \subseteq \gamma(a_1)$.
\end{theorem}

\begin{theorem} Let $a_0$ be the iterative MPC Source and let $a_1$ be the vectorized MPC Source computed by Basic vectorization. 
We have that $a_0 \le a_1$. 
\end{theorem}

Theorem 1 states that a def-use edge in MPC Source $a_0$ gives rise to a set of def-use edges in $\gamma(a_0)$
and a ``same'' edge in $a_1$ gives rise to a set of def-use edges in $\gamma(a_1)$ and the former set is included in the latter one.
This means that the computation of $\gamma(a_0)$ is carried out by $\gamma(a_1)$ as well, computing the same result. 
The theorem allows that $a_1$ and $\gamma(a_1)$ have larger state that $a_0$ and $\gamma(a_0)$, i.e., doing additional 
computation into additional memory locations. Theorem 2 establishes the correctness of vectorization --- 
clearly, vectorization preserves all statements and def-use edges in the original MPC Source, and therefore the linearization computes the 
same result as the original MPC Source. (In this case we have an equivalence, $a_0 = a_1$ according to our partial order.)

\subsection{Extension with Array Writes}
\label{sec:array_writes}

\subsubsection{Removal of Infeasible Edges}

Array writes limit vectorization as they sometimes introduce infeasible loop-carried dependencies. Consider the classical example from~\cite{Aiken:1988}: 


{\small
\begin{pythonn}
for i in range(N):
  A[i] = B[i] + 10;
  B[i] = A[i] * D[i-1];
  C[i] = A[i] * D[i-1];
  D[i] = B[i] * C[i];
\end{pythonn}
}

In Cytron's SSA this code (roughly) translates into

{\small
\begin{pythonn}
for i in range(N):
  A_1 = PHI(A_0,A_2)
  B_1 = PHI(B_0,B_2)
  C_1 = PHI(C_0,C_2)
  D_1 = PHI(D_0,D_2)
  A_2 = update(A_1, i, B_1[i] + 10);
  B_2 = update(B_1, i, A_2[i] * D_1[i-1]);
  C_2 = update(C_1, i, A_2[i] * D_1[i-1]);
  D_2 = update(D_1, i, B_2[i] * C_2[i]);
\end{pythonn}
}

There is a cycle around {\sf B\_1 = PHI(B\_0,B\_2)} that includes statement
{\sf A\_1 = update(A\_0, i, B\_1[i] + 10)} and that statement won't be vectorized even
though in fact there is no loop-carried dependency from the write of {\sf B\_1[i]} at 7 to the
read of {\sf ... = B\_1[i]} at 6.

The following algorithm removes certain infeasible loop-carried dependencies that are due to array writes. Consider a loop with index $0 \le j < J$
nested at $i,j,k$. Here $i$ represents the enclosing loops of $j$ and $k$ represents the enclosed loops in $j$.

\begin{algorithmic}
\FOR {each array ${\sf A}$ written in loop $j$}
\STATE \COMMENT { including enclosed loops in $j$ }
\STATE dep = False
\FOR {each pair def: ${\sf A}_m[f(i,j,k)] = ... $, and use: $ ... = {\sf A}_n[f'(i,j,k)]$ in loop $j$}
\IF {$\exists \underline{i}, \underline{j}, \underline{j}', \underline{k}, \underline{k}'$, s.t. $0\! \le\! \underline{i}\! <\! I,\: 0\! \le\! \underline{j}, \underline{j}' \! <\! J,\: 0\! \le\! \underline{k}, \underline{k}' \! <\! K,\: \underline{j}<\underline{j}'$, and $f(\underline{i},\underline{j},\underline{k}) = f'(\underline{i},\underline{j}',\underline{k}')$}
\STATE dep = True
\ENDIF
\ENDFOR
\IF {dep == False}
\STATE remove back edge into ${\sf A}$'s $\phi$-node in loop $j$.
\ENDIF
\ENDFOR
\end{algorithmic}

Consider a loop $j$ enclosed in some fixed $\underline{i}$. Only if an update (definition) ${\sf A}_m[f(i,j,k)] = ... $ at some iteration $\underline{j}$
references the \emph{same} array element as a use $ ... = {\sf A}_n[f'(i,j,k)]$ at some later iteration $\underline{j}'$,
we may have a loop-carried dependence for ${\sf A}$ due to this def-use pair. (In contrast, Cytron's algorithm inserts a loop-carried dependency every time there is an array update.)
The algorithm above examines all def-use pairs in loop $j$, including defs and uses in nested loops, searching for values $\underline{i}, \underline{j}, \underline{j}', \underline{k}, \underline{k}'$ that satisfy
$f(\underline{i},\underline{j},\underline{k}) = f'(\underline{i},\underline{j}',\underline{k}')$. If such values exist for some def-use pair, then there is a potential
loop-carried dependence on ${\sf A}$; otherwise there is not and we can remove the spurious backward edge thus ``freeing up'' statements for vectorization. %\ana{Double check this!!!}

Consider the earlier example. There is a single loop, $i$. Clearly, there is no pair $\underline{i}$ and $\underline{i}'$, where $\underline{i} < \underline{i}'$ that make $\underline{i} = \underline{i}'$
due to the def-use pairs of {\sf A} 6-7 and 6-8.
Therefore, we remove the back edge from 6 to the phi-node 2. Analogously, we remove the back edges from 7 to 3 and from 8 to 4. However, there are many values $\underline{i} < \underline{i'}$ that make $\underline{i} = \underline{i'}-1$ and the back edge from 9 to 5 remains (def-use pairs for {\sf D}). As a result of removing these spurious edges, Vectorization will find that statement 6 is vectorizable. Statements 7, 8 and 9 will correctly appear in the FOR loop.

Note however, that this step renders some array phi-nodes target-less. We handle target-less phi-nodes with a minor extension of Vectorization (Phase 2).
First, we merge closures that update the same array. This simplifies handling of array $\phi$-nodes: if each closure is turned into a separate loop
each loop will need to have its own array phi-node to account for the update and this would complicate the analysis.
Second, we add the target-less node of array {\sf A} back to the closure that updates {\sf A} ---
the intuition is, even if there is no loop-carried dependence from writes to reads on {\sf A}, {\sf A} is written and the write (i.e., update) cannot be vectorized;
therefore, the updated array has to carry to the next iteration of the loop. Third, in cases when the phi-node remains target-less, i.e., cases when the
array write can be vectorized, we have to properly remove the phi-node replacing uses of the left-hand side of the phi-node with its arguments.

%\ana{Commented out previous writeup. Removal/handling of targetless phi-nodes will go into the Extension to Basic vectorization.}


\begin{comment}
\subsubsection{Array MUX refinement}

\ana{TODO: WIP, get to BV without MUX refinement for now? For now, skip MUX refinement.}

Next, the algorithm refines array MUX statements.
MPC-source after Cytron's SSA may result in statements $\texttt{A}_j = \emph{MUX}(..., \texttt{A}_k, \texttt{A}_l)$, which imply that any index
of $A$ can be written at this point and therefore there is a loop-carried dependency. In some cases the MUX can be refined to just a single
index or a pair of indices, e.g., $\texttt{A}_j[i] = \emph{MUX}(c, \texttt{A}_k[i], \texttt{A}_l[i])$.

%\ana{Clarify why this helps. Still work in progress.}
This is to reduce the dimensionality of simd-ified computation. Technically, $\texttt{A}_j = \emph{MUX}(..., \texttt{A}_k, \texttt{A}_l)$ is
a simdified operation that can be carried out in parallel "in one round". However, particularly when \texttt{A} is a multi-dimensional array, there is
substantial increase in the size of the arrays (vectors) we send to SIMD operations. Refining to an update to a specific index would reduce
the size of those vectors. Note that this is a heuristic that handles a common case, but not all cases of array updates.
%\ana{TODO: Simplify this algorithm, taking into account the restriction to canonical updates. It should be handling \_all\_ cases.}


\begin{algorithmic}
\FOR {each $\mathit{stmt}$: $\texttt{A}_j = \emph{MUX}(c, \texttt{A}_k, \texttt{A}_l)$ in the MPC-source seq.}
\STATE $i_1 = \mathit{find\_update}(A_k)$ \COMMENT { Is null when $A_k = \phi(...)$ }
\STATE $i_2 = \mathit{find\_update}(A_l)$ \COMMENT { Is null when $A_l = \phi(...)$ }
\IF {$i_1 == i_2$ or $i_1$ is null or $i_2$ is null} \STATE \COMMENT{ With our restrictions on writes we must have $i_1 = i_2$. }
\STATE replace $\mathit{stmt}$ with \\
\STATE $\texttt{A}_j = \texttt{A}_{j-1}$; $\texttt{A}_j[i_1] = \emph{MUX}(c, \texttt{A}_k[i_1], \texttt{A}_l[i_1])$
%\ELSIF {$i_1 \ncong i_2$}
%\STATE replace $\mathit{stmt}$ with \\
%\STATE $\texttt{A}_j = \texttt{A}_k$; $\texttt{A}_j[i_1] = \emph{MUX}(c, \texttt{A}_k[i_1], \texttt{A}_l[i_1])$
\ELSE
\STATE $\mathit{stmt}$ stays as is
\ENDIF
\ENDFOR
\end{algorithmic}
\end{comment}

\subsubsection{Restricting Array Writes}

For now, we restrict array updates to \emph{canonical updates}. Assume (for simplicity) a two-dimensional array {\sf A[I,J]}. A canonical update is the following:
\begin{pythonn}
for i in range(I):
  for j in range(J):
    ...
    A[i,j] = ...
    ...
\end{pythonn}

The update {\sf A[i,j]} can be nested into an inner loop and there may be multiple updates, i.e., writes to {\sf A[i,j]}. However, update such as {\sf A[i-1,j] = ...} or {\sf A[i-1,j-1] = ...}, etc., is not allowed. Additionally, while there could be several different loops that perform canonical updates, they must be of the same dimensionality, i.e., an update of higher or lower dimension, e.g., {\sf A[i,j,k] = ...} is not allowed. We compute the \emph{canonical dimensionality} of each write array by examining the array writes in the original program and rejecting programs that violate the canonical write restriction. This restriction simplifies reasoning in this early stage of the compiler; we will look to relax the restriction in future work.

Another restriction/assumption is that we assume the output array is given as input with initial values, and it is of size consistent with its canonical dimensionality.
%\ana{This restriction will require that we update our benchmarks that come with output arrays.}

Reads through an arbitrary formula, such as {\sf A[i-1]} for example, are allowed; currently, the projection function returns dummy values if the read formula is out of bounds; we assume the programmer ensures that the program still computes correct output in this case.

\subsubsection{Changes to Basic Vectorization}

In addition to the changes for the handling of target-less phi-nodes, Basic Vectorization has to handle def-use edges $X \rightarrow Y$ where $X$ defines and $Y$ uses an array variable. The definition can be an update {\sf A\_2 = update(A\_1,i,...)}, a pseudo $\phi$-node {\sf A\_2 = PHI(A\_0,A\_1)}, etc.. Note that $\phi$-nodes for arrays have no subscript operations the way there are subscript operations in analysis-introduced arrays representing scalars. While there are variations, the most intuitive implementation will perform Basic Vectorization Phase 1 as is, inserting
$\mathit{raise\_dim}$ and $\mathit{drop\_dim}$ in the same way. However, the implementation of raise dimension and drop dimension will be adapted because the dimension cannot be raised or dropped to a dimension lower than the canonical one. Consider a def-use edge $X \rightarrow Y$ for an array ${\sf A}$.

\begin{enumerate}

\item same-level $X \rightarrow Y$. Do nothing, propagate the array, which happens to be of the right dimension.
%\ana{There might be some opportunities to do copy propagation optimization and save some cycles, but let's leave this for later.}

\item inner-to-outer $X \rightarrow Y$ triggers the addition of $\mathit{drop\_dim}$. However, the dimensionality cannot be dropped below the canonical dimensionality of the array. E.g., if the dimensionality of the loop enclosure $X$ is already at the canonical one, then $\mathit{drop\_dim}$ has no effect.

\item outer-to-inner $X \rightarrow Y$ riggers  $\mathit{raise\_dim}$. Again, if the dimensionality of the loop enclosure of $Y$ is smaller or same as the canonical dimensionality of the array, then it has no effect, otherwise, if dimensionality is greater than the canonical dimensionality, $\mathit{raise\_dim(...)}$ (at $X$) is the same as in Basic Vectorization.

\item "mixed" $X \rightarrow Y$. We assume that the mixed edge is transformed into an inner-to-outer followed by outer-to-inner edge before we perform vectorization, just as with Basic vectorization.

\end{enumerate}

If the use of the array is a read ${\sf A}[f(i,j,k)]$ different than a canonical read ${\sf A}[i,j,k]$, then we need to add a reshape operation as
all arrays are ${\sf A}[i,j,k]$. It can be added after raise\_dim/drop\_dim or incorporated in these operations.
The bulk of the change is in Phase 2 of Vectorization as outlined earlier.

\subsubsection{Examples with Array Writes}

\paragraph{Example 1}

First, the canonical dimensionality of all {\sf A,B,C} and {\sf D} is 1.
After Phase 1 of Vectorization the Aiken's array write example will be (roughly) as follows:

{\small
\begin{pythonn}
for i in range(N):
  A_1 = PHI(A_0,A_2)
  B_1 = PHI(B_0,B_2)
  C_1 = PHI(C_0,C_2)
  D_1 = PHI(D_0,D_2)
  A_2 = update(A_1, I, B_1[I] + 10);
  B_2 = update(B_1, I, A_2[I] * D_1[I-1]);
  C_2 = update(C_1, I, A_2[I] * D_1[I-1]);
  D_2 = update(D_1, I, B_2[I] * C_2[I]);
\end{pythonn}
}

Note that since all def-uses are same-level (i.e., reads and writes of the array elements)
no raise dimension or drop dimension happens.

Phase 2 computes the closure of 5; $cl = \{5,7,8, 9\}$
while 6 is vectorizable. Recall that 2,3, and 4 are target-less phi-nodes. Since the closure $cl$ includes updates to {\sf B} and {\sf C},
the corresponding phi-nodes are added back to the closure and the def-use edges are added back to the target-less nodes. The uses of {\sf A\_1} and {\sf B\_1} in the vectorized statement
turn into uses of {\sf A\_0} and {\sf B\_0} respectively; this is done for all original target-less phi-node. (But note that {\sf A\_0} is irrelevant; the update writes into array {\sf A\_2} in parallel.)
Finally, the target-less phi-node for {\sf A} is discarded.

{\small
\begin{pythonn}
A_2 = update(A_0, I, ADD_SIMD(B_0[I],10));
   equiv. to A_2[I] = ADD_SIMD(B_0[I],10)
for i in range(N): // MOTION loop
  B_1 = PHI(B_0,B_2)
  C_1 = PHI(C_0,C_2)
  D_1 = PHI(D_0,D_2)
  B_2 = update(B_1, i, A_2[i] * D_1[i-1]);
     equiv. to B_2 = B_1; B_2[i] = A_2[i] * D_1[i-1];
  C_2 = update(C_1, i, A_2[i] * D_1[i-1]);
  D_2 = update(D_1, i, B_2[i] * C_2[i]);
\end{pythonn}
}



\begin{comment}
\begin{algorithmic}
\STATE 1. $\texttt{A}_2 = update(\texttt{A}_0; \stackrel{\rightarrow}{i}, \mathit{ADD\_SIMD}(\texttt{B}_0[\stackrel{\rightarrow}i],[10,...])$ \COMMENT{Fully vectorized, size N.}
\STATE FOR i=0; i<N; i++; \COMMENT{ MOTION loop }
\STATE 2. $\texttt{B}_1$ = $\phi(\texttt{B}_0,\texttt{B}_2)$
\STATE 3. $\texttt{C}_1$ = $\phi(\texttt{C}_0,\texttt{C}_2)$
\STATE 4. $\texttt{D}_1$ = $\phi(\texttt{D}_0,\texttt{D}_2)$
\STATE 5. $\texttt{B}_2 = update(\texttt{B}_1, i, \mathit{MUL}(\texttt{A}_2[i], \texttt{D}_1[{i-1}])$
\STATE 6. $\texttt{C}_2 = update(\texttt{C}_1, i, \mathit{MUL}(\texttt{A}_2[i], \texttt{D}_1[{i-1}])$
\STATE 7. $\texttt{D}_2 = update(\texttt{D}_1, i, \mathit{MUL}(\texttt{B}_2[i], \texttt{C}_1[i])$
\end{algorithmic}
\end{comment}

\paragraph{Example 2} Now consider the MPC Source of Histogram:

{\small
\begin{pythonn}
for i in range(0, num_bins):
  res1 = PHI(res, res2)
  for j in range(0, N):
    res2 = PHI(res1, res3)
    tmp1 = (A[j] == i)
    tmp2 = (res2[i] + B[j])
    tmp3 = MUX(tmp1, res2[i], tmp2)
    res3 = Update(res2, i, tmp3)
return res1
\end{pythonn}
}

The canonical dimensionality of {\sf res} is 1. Also, the phi-node {\sf res1 = PHI(res, res2)} is a target-less phi-node (the implication being that the inner for loop can be vectorized across $i$). After Phase 1, Vectorization produces the following code (statements are implicitly vectorized along $i$ and $j$). In a vectorized update statement, we can ignore the incoming array, {\sf res2} in this case. The update writes (in parallel) all locations of the 2-dimensional array, in this case it sets up each {\sf res3[i,j] = tmp3[i,j]}.

{\small
\begin{pythonn}
A1 = raise_dim(A, j, ((i:num_bins),(j:N)))
B1 = raise_dim(B, j, ((i:num_bins),(j:N)))
I = raise_dim(i, ((i:num_bins),(j:N)))
for i in range(0, num_bins):
   res1 = PHI(res, res2^) # target-less phi-node
   res1^ = raise_dim(res1, (j:N))
   for j in range(0, N):
     res2 = PHI(res1^, res3)
     tmp1 = (A1 == I)
     tmp2 = (res2 + B1)
     tmp3 = MUX(tmp1, res2, tmp2)
     res3 = Update(res2, (I,J), tmp3)
   res2^ = drop_dim(res2)
res1'' = drop_dim(res1)
return res1''
\end{pythonn}
}

Processing the inner loop in Phase 2 vectorizes {\sf tmp1 = (A1 == I)} along the $j$ dimension but leaves the rest of the statements in a MOTION loop. Processing the outer loop is interesting. This is because the PHI node is a target-less node, and therefore, there are no closures! Several things happen. (1) Everything can be vectorized along the $i$ dimension. (2) We remove the target-less PHI node, however, we must update uses of {\sf res1} appropriately: the use at $\mathit{raise\_dim}$ goes to the first argument of the PHI function and the use at $\mathit{drop\_dim}$ goes to the second argument.

{\small
\begin{pythonn}
A1 = raise_dim(A, j, ((i:num_bins),(j:N)))
B1 = raise_dim(B, j, ((i:num_bins),(j:N)))
I1 = raise_dim(i, ((i:num_bins),(j:N)))

tmp1[I,J] = (A1[I,J] == I1[I,J])

res1^ = raise_dim(res, (j:N)) // replacing res1 with res, 1st arg
for j in range(0, N):
   res2 = PHI(res1^, res3)
   tmp2[I,j] = (res2[I,j] + B1[I,j])
   tmp3[I,j] = MUX(tmp1[I,j], res2[I,j], tmp2[I,j])
   res3 = Update(res2, (I,j), tmp3)
   equiv. to res3 = res2; res3[I,j] = tmp3[I,j]
res2^ = drop_dim(res2)
res1 = drop_dim(res2^) // replacing with res2^, 2nd arg. NOOP
return res1
\end{pythonn}
}

