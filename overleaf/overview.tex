\section{Overview}
\label{sec:overview}


\begin{table*}
\begin{tabular}{ccc}
\begin{minipage}{0.275\textwidth}
{\small
\begin{pythonn}
def biometric(C: shared[list[int]], D: int,
      S: shared[list[int]], N: int) ->
      shared[tuple[int,int]]:
   min_sum : int = MAX_INT
   min_idx : int = 0
   for i in range(N):
      sum : int = 0
      for j in range(D):
         # d = S[i,j] - C[j]
         d : int = S[i * D + j] - C[j]
         p : int = d * d
         sum = sum + p
      if sum < min_sum:
         min_sum : int = sum
         min_idx : int = i
   return (min_sum, min_idx)
\end{pythonn}
}
\end{minipage}

&

\begin{minipage}{0.33\textwidth}
{\tiny
\begin{pythonn}
min_sum!1 = MAX_INT
min_idx!1 = 0
for i in range(0, N):
   min_sum!2 = PHI(min_sum!1, min_sum!4)
   min_idx!2 = PHI(min_idx!1, min_idx!4)
   sum!2 = 0
   for j in range(0, D):
      sum!3 = PHI(sum!2, sum!4)
      d = SUB(S[((i * D) + j)],C[j])
      p = MUL(d,d)
      sum!4 = ADD(sum!3,p)
   t = CMP(sum!3,min_sum!2)
   min_sum!3 = sum!3
   min_idx!3 = i
   min_sum!4 = MUX(t, min_sum!3, min_sum!2)
   min_idx!4 = MUX(t, min_idx!3, min_idx!2)
return (min_sum!2, min_idx!2)
\end{pythonn}
}
\end{minipage}

&

\begin{minipage}{0.33\textwidth}
{\small
\begin{pythonn}
min_sum!1 = MAX_INT
min_idx!1 = 0
# S^ is same as S. C^ replicates C N times:
S^ = raise_dim(S, ((i * D) + j), (i:N,j:D)) #S^[i,j] = S[i,j]
C^ = raise_dim(C, j, (i:N,j:D)) #C^[i,j] = C[j]

sum!2[I] = [0,..,0]
# computes _all_  "at once"
d[I,J] = SUB_SIMD(S^[I,J],C^[I,J])
p[I,J] = MUL_SIMD(d[I,J],d[I,J])

for j in range(0, D):
   # sum!2[I], sum!3[I], sum!4[I] are size-N vectors
   # computes N intermediate sums "at once"
   sum!3[I] = PHI(sum!2[I], sum!4[I])
   sum!4[I] = ADD_SIMD(sum!3[I],p[I,j])

min_idx!3[I] = [0,1,...N-1]
for i in range(0, N):
   min_sum!2 = PHI(min_sum!1, min_sum!4)
   t[i] = CMP(sum!3[i],min_sum!2)
   min_sum!4 = MUX(t[i], sum!3[i], min_sum!2)
for i in range(0, N):
   min_idx!2 = PHI(min_idx!1, min_idx!4)
   min_idx!4 = MUX(t[i], min_idx!3[i], min_idx!2)
return (min_sum!2, min_idx!2)
\end{pythonn}
}
\end{minipage}

\\

(a) IMP Source & (b) MPC Source & (c) Optimized MPC Source
\end{tabular}
\caption{Biometric Matching: ==== From (a) IMP Source to (b) MPC Source: 
First, MPC Source is an SSA form.
Second, it is linear. The conditional in lines 13-14 in IMP Source turns into the linear code in lines 12-16 in MPC Source.
The test turns into the CMP operation {\sf t = CMP(sum!3,min\_sum!2)}, followed by the
true-branch sequence, followed by the MUX operations. The first MUX operation selects the value
of {\sf min\_sum}: if {\sf t} is true, then {\sf min\_sum} gets the value of the second multiplexer
argument,  {\sf min\_sum!3}, otherwise it takes the value of the third argument, {\sf min\_sum!2}.
Third, MPC Source is a special form of SSA. The SSA $\phi$-nodes at the if-then-else (lines 13-15) turn into
MUX operations, while the $\phi$-nodes at for-loops turn into \emph{pseudo} PHI nodes with a straightforward semantics.
==== From (b) MPC Source to (c) Optimized MPC Source: 
The compiler determines that SUB and MUL in ``naive'' MPC Source (lines 9 and 10 in (b))
can be fully vectorized into the SIMD SUB and MUL in optimized MPC Source (lines 9 and 10 in (c)).
Notation {\sf p[I,J]} denotes a 2-dimensional array with fully vectorized dimensions.
The computation of sum (line 11 in (b))
is sequential across the $j$-dimension, but it is parallel across the $i$-dimension.
The loop in lines 12-16 in (c) illustrates; here {\sf p[I,j]} refers to the $j$-th column in {\sf p}.
Unfortunately, CMP and MUX remain sequential.
% \ana{Add explanation to caption, this is hard to see.}
}
\label{tab:source_and_MPC_source_and_optimized_MPC_source}
\end{table*}


\subsection{Source}

As a running example, consider Biometric matching, a standard MPC benchmark.
An intuitive (and naive) implementation is as shown in Listing~\ref{tab:source_and_MPC_source_and_optimized_MPC_source}(a).
Array {\sf C} is the feature vector of {\sf D} features that we wish to match and {\sf S}
is the database of {\sf N} size-{\sf D} vectors that we match against.

Our compiler takes essentially standard IMP~\cite{Nipkow2014}
syntax and imposes certain semantic restrictions. %(We detail the restrictions in the following sections.)
The programmer writes an iterative program and annotates certain inputs
and outputs as \emph{shared}. In the example arrays {\sf C} and {\sf S}
are \texttt{shared}, meaning that they store shares, however, the array sizes {\sf D} and
{\sf N} respectively are plaintext. 
%The code iterates over the entries in the
%database and computes the Euclidean distance of the current
%entry ${\sf S}[i]$ and {\sf C} (its square actually). The program returns the index of the vector that gives
%the best match plus the corresponding sum of squares.

\subsection{MPC Source and Cost of Schedule}

Our compiler generates an intermediate representation, MPC Source. 
MPC Source is a \emph{linear} SSA form.
MPC Source for Biometric Matching is shown and described in detail in
Listing~\ref{tab:source_and_MPC_source_and_optimized_MPC_source}(b). 


\begin{comment}
First, MPC Source is an SSA form.
Second, it is linear. The conditional in lines 13-14 in IMP Source turns into the linear code in lines 12-16 in MPC Source.
The test turns into the CMP operation {\sf t = CMP(sum!3,min\_sum!2)}, followed by the
true-branch sequence, followed by the MUX operations. The first MUX operation selects the value
of {\sf min\_sum}: if {\sf t} is true, then {\sf min\_sum} gets the value of the second multiplexer
argument,  {\sf min\_sum!3}, otherwise it takes the value of the third argument, {\sf min\_sum!2}.
Third, MPC Source is a special form of SSA. The SSA $\phi$-nodes at the if-then-else (lines 13-15) turn into
MUX operations, while the $\phi$-nodes at for-loops turn into \emph{pseudo} PHI nodes with a straightforward semantics.
\end{comment}

We turn to our analytical model to compute the \emph{cost} of the iterative program. Assume
cost $\beta$ for a local MPC operation (e.g., ADD in Arithmetic sharing) and cost $\alpha$ for a remote
MPC operation (e.g., MUX, CMP, etc.). Assuming that ADD is $\beta$ and SUB, CMP and MUX are $\alpha$, 
the MPC Source in Listing~\ref{tab:source_and_MPC_source_and_optimized_MPC_source}(b) gives 
rise to an iterative schedule with cost $ND(2\alpha+\beta) + N(3\alpha$).

A key contribution is the vectorizing transformation. We can compute all $N*D$
subtractions (line 9 in (b)) in a single SIMD instruction; similarly we can compute
all multiplications (line 10) in a single SIMD instruction. And while computation
of an individual sum remains sequential, we can compute the $N$ sums in parallel.
%Our compiler \emph{automatically detects these opportunities and transforms the program}.
%It is standard that MPC researchers write vectorized versions of the Biometric
%program by hand; we are the first (to the best of our knowledge) to automatically
%transform a naive, iterative MPC program into an unintuitive vectorized one.

\subsection{Vectorized MPC Source and Cost of Schedule}

Our compiler produces the vectorized program shown and described in 
Listing~\ref{tab:source_and_MPC_source_and_optimized_MPC_source}(c).
Note that this is still our intermediate representation, Optimized MPC Source. Subsequently,
the compiler turns this code into MOTION variables, loops and SIMD primitives, which MOTION then
uses to generate the circuit.
\begin{comment}
The compiler determines that SUB and MUL in ``naive'' MPC Source (lines 9 and 10 in (b))
can be fully vectorized into the SIMD SUB and MUL in optimized MPC Source (lines 9 and 10 in (c)).
Notation {\sf p[I,J]} denotes a 2-dimensional array with fully vectorized dimensions.
The computation of sum (line 11 in (b))
is sequential across the $j$-dimension, but it is parallel across the $i$-dimension.
The loop in lines 12-16 in (c) illustrates; here {\sf p[I,j]} refers to the $j$-th column in {\sf p}.
Unfortunately, CMP and MUX remain sequential.
\end{comment}

In MPC back ends, executing $n$ operations ``at once'' in a single SIMD operation costs a lot less than executing those $n$ operations one by one.
This is particularly important when there is communication (i.e., in remote), since many 1-bit values are sent at once rather than sequentially.
%In MPC compilers a vectorized operation that computing M operations "at once" costs
%essentially the same ($\alpha$ or $\beta$) as an individual operation.
We elaborate on the cost model in Section~\secref{sec:model} but for now consider that
each operation has a \emph{fixed} portion (does benefits from amortization) and
a \emph{variable} portion (does not benefit from amortization): $\alpha = \alpha_\mathit{fix} + \alpha_\mathit{var}$.
This gives rise to the following formula for amortized cost: $f(n) = \alpha_\mathit{fix} + n\alpha_\mathit{var}$,
as opposed to unamortized cost $g(n) = n\alpha_\mathit{fix} + n\alpha_\mathit{var}$. We extend the same reasoning to
$\beta$-instructions.

Thus, the fixed cost of the vectorized program amounts to
$2\alpha_\mathit{fix}$ + $D \beta_\mathit{fix} + N(3\alpha_\mathit{fix})$. (The variable cost is the same in both the vectorized and non-vectorized programs.)
The first term in the sum corresponds to the vectorized
subtraction and multiplication (lines 9-10 in (c)), the second term corresponds to the for-loop on $j$ (lines 12-16) and the third
one corresponds to the remaining for-loops on $i$ (lines 19-25). 
Clearly, $2\alpha_\mathit{fix} + D\beta_\mathit{fix} + N(3\alpha_\mathit{fix}) << ND(2\alpha_\mathit{fix}+\beta_\mathit{fix}) + N3\alpha_\mathit{fix}$.
Empirically, we observe that (1) $\alpha_\mathit{var} \approx 0$ and (2) there is orders of magnitude improvement in running time and memory. 
E.g., we see about 12x improvement in online time in GMW for $N=128$. Additionally, the non-vectorized version runs out of memory for $N=256$, 
while the vectorized one runs with the standard maximal input size $N=4,096$.
%\ana{Edit numbers with final experiments.}