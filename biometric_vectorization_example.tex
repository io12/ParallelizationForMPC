%\documentclass{article}

%\begin{document}

\subsection{Example: biometric}

We start from Benjamin's code with linear loops (MPC Source):

{\small
\begin{verbatim}
min_sum!1 = 10000
min_index!1 = 0
for i in range(0, N!0):
   min_sum!2 = PHI(min_sum!1, min_sum!4)
   min_index!2 = PHI(min_index!1, min_index!4)
   sum!2 = 0
   for j in range(0, D!0):
     sum!3 = PHI(sum!2, sum!4)
     d!3 = (S!0[((i * D!0) + j)] - C!0[j])
     p!3 = (d!3 * d!3)
     sum!4 = (sum!3 + p!3)
   !1!2 = (sum!3 < min_sum!2)
   min_sum!3 = sum!3
   min_index!3 = i
   min_sum!4 = MUX(!1!2, min_sum!3, min_sum!2)
   min_index!4 = MUX(!1!2, min_index!3, min_index!2)
!2!1 = (min_sum!2, min_index!2)   
\end{verbatim}
}

\subsubsection{Phase 1 of Basic Vectorization}

The transformation preserves the dependence edges. It raises the dimensions of scalars and optimistically vectorizes all operations. 
The next phase discovers loop-carried dependences and removes affected vectorization.

In the code below, all initializations (e.g., \texttt{min\_sum!3 = i}), operations, and PHI nodes are \emph{implicitly vectorized}. 
$\mathit{raise\_dim}$ and $\mathit{drop\_dim}$ statements, as well as propagation statements (e.g., \texttt{min\_sum!3 = sum!3\^}) 
may have slightly different interpretation. \ana{This is vague, may need elaboration.}

Note the two different versions of $\mathit{raise\_dim}$. \ana{More here! One just adds a least-significant dimension. The other one may reshape an input array.}

{\small
\begin{verbatim}
min_sum!1 = 10000
min_sum!1^ = raise_dim(min_sum!1, (i:N!0))
min_index!1 = 0
min_index!1^ = raise_dim(min_index!1, (i:N!0))
S!0^ = raise_dim(S!0, ((i * D!0) + j), (i:N!0,j:D!0))
C!0^ = raise_dim(C!0, j, (i:N!0,j:D!0))
for i in range(0, N!0):
   min_sum!2 = PHI(min_sum!1^, min_sum!4)
   min_index!2 = PHI(min_index!1^, min_index!4) 
   sum!2 = 0 // Will lift, when hoisted
   sum!2^ = raise_dim(sum!2, (j:D!0)) // Special form?
   for j in range(0, D!0):
      sum!3 = PHI(sum!2^, sum!4)
      d!3 = S!0^ - C!0^
      p!3 = (d!3 * d!3) 
      sum!4 = (sum!3 + p!3)
   sum!3^ = drop_dim(sum!3)     
   !1!2 = (sum!3^ < min_sum!2)
   min_sum!3 = sum!3^
   min_index!3 = i  // Same-level, will lift when hoisted
   min_sum!4 = MUX(!1!2, min_sum!3, min_sum!2)
   min_index!4 = MUX(!1!2, min_index!3, min_index!2)
min_sum!2^ = drop_dim(min_sum!2)
min_index!2^ = drop_dim(min_index!2)   
!2!1 = (min_sum!2^, min_index!2^)     
\end{verbatim}
}

\subsubsection{Phase 2 of Basic Vectorization}

This phase analyzes statements from the innermost loop to the outermost. The key point is to discover loop-carried dependencies and re-introduce loops whenever dependencies make this necessary.

Starting at the inner phi-node \texttt{sum!3 = PHI(sum!2\^, sum!4)}, the algorithm first computes its closure. The closure amounts to the phi-node itself and the addition node \texttt{sum!4 = (sum!3 + p!3)}, accounting for the loop-carried dependency of the computation of \texttt{sum}. The algorithm replaces this closure with a FOR loop on $j$ removing vectoriaztion on $j$. Note that the SUB and MUL computations remain outside of the loop as they do not depend on phi-nodes that are part of cycles. The dependences are from \texttt{p!3[I,J] = (d!3[I,J] * d!3[I,J])} to the monolithic FOR loop and from the FOR loop to \texttt{sum!3\^ = drop\_dim(sum!3)}. (Lower case index, e.g., \texttt{i}, indicates non-vectorized dimension, while uppercase index, e.g., \texttt{I} indicates vectorized dimension.)

After processing inner loop code becomes:

{\small
\begin{verbatim}
min_sum!1 = 10000
min_sum!1^ = raise_dim(min_sum!1, (i:N!0))
min_index!1 = 0
min_index!1^ = raise_dim(min_index!1, (i:N!0))
S!0^ = raise_dim(S!0, ((i * D!0) + j), (i:N!0,j:D!0))
C!0^ = raise_dim(C!0, j, (i:N!0,j:D!0))
for i in range(0, N!0):
  min_sum!2[I] = PHI(min_sum!1^[I], min_sum!4[I]) 
  min_index!2[I] = PHI(min_index!1^[I], min_index!4[I])  
  sum!2 = [0,..,0] 
  sum!2^ = raise_dim(sum!2, (j:D!0))
  d!3[I,J] = S!0^[I,J] - C!0^[I,J]
  p!3[I,J] = (d!3[I,J] * d!3[I,J])
  for j in range(0, D!0):
    sum!3[I,j] = PHI(sum!2^[I,j], sum!4[I,j-1])       
    sum!4[I,j] = (sum!3[I,j] + p!3[I,j])
  sum!3^ = drop_dim(sum!3)     
  !1!2[I] = (sum!3^[I] < min_sum!2[I])
  min_sum!3 = sum!3^
  min_index!3 = i
  min_sum!4[I] = MUX(!1!2[I], min_sum!3[I], min_sum!2[I])
   min_index!4[I] = MUX(!1!2[I], min_index!3[I], min_index!2[I])
min_sum!2^ = drop_dim(min_sum!2)
min_index!2^ = drop_dim(min_index!2)   
!2!1 = (min_sum!2^, min_index!2^)
\end{verbatim}
}

When processing the outer loop two closures arise, one for \texttt{min\_sum!2[I] = PHI(...)} and one 
for \texttt{min\_index!2[I] = PHI(...)}. Since the two closures \emph{do not} intersect, we have two distinct FOR-loops on $i$:

{\small
\begin{verbatim}
min_sum!1 = 10000
min_sum!1^ = raise_dim(min_sum!1, (i:N!0))
min_index!1 = 0
min_index!1^ = raise_dim(min_index!1, (i:N!0))
S!0^ = raise_dim(S!0, ((i * D!0) + j), (i:N!0,j:D!0))
C!0^ = raise_dim(C!0, j, (i:N!0,j:D!0))

sum!2 = [0,..,0]
sum!2^ = raise_dim(sum!2, (j:D!0))
d!3[I,J] = S!0^[I,J] - C!0^[I,J]
p!3[I,J] = (d!3[I,J] * d!3[I,J])

for j in range(0, D!0):
  sum!3[I,j] = PHI(sum!2^[I,j], sum!4[I,j-1])       
  sum!4[I,j] = (sum!3[I,j] + p!3[I,j])

sum!3^ = drop_dim(sum!3)     
min_index!3 = [0,1,2,...N!0-1] // or min_index!3 = [i, (i:N!0)]
min_sum!3 = sum!3^

for i in range(0, N!0):
  min_sum!2[i] = PHI(min_sum!1^[i], min_sum!4[i-1]) 
  !1!2[i] = (sum!3^[i] < min_sum!2[i])
  min_sum!4[i] = MUX(!1!2[i], min_sum!3[i], min_sum!2[i])
    
for i in range(0, N!0):
  min_index!2[i] = PHI(min_index!1^[i], min_index!4[i-1])  
   min_index!4[i] = MUX(!1!2[i], min_index!3[i], min_index!2[i])

min_sum!2^ = drop_dim(min_sum!2)
min_index!2^ = drop_dim(min_index!2)   
!2!1 = (min_sum!2^, min_index!2^)
\end{verbatim}
}



\subsubsection{Phase 3 of Basic Vectorization}

This phase removes redundant dimensionality. 
It starts by removing redundant dimensions in MOTION loops followed
by removal of redundant drop dimension statements.
It then does (extended) constant propagation 
to "bypass" raise statements, followed by copy propagation
and dead code elimination.

The code becomes closer to what we started with:

{\small
\begin{verbatim}
min_sum!1 = 10000
min_index!1 = 0
S!0^ = raise_dim(S!0, ((i * D!0) + j), (i:N!0,j:D!0))
C!0^ = raise_dim(C!0, j, (i:N!0,j:D!0))

sum!2 = [0,..,0]
d!3[I,J] = S!0^[I,J] - C!0^[I,J]
p!3[I,J] = (d!3[I,J] * d!3[I,J])

// j is redundant for sum!3 and sum!4
for j in range(0, D!0):
  sum!3[I] = PHI(sum!2^[I], sum!4[I])       
  sum!4[I] = (sum!3[I] + p!3[I,j])

// drop_dim is redundant, removing
// then copy propagation and dead code elimination 
min_index!3 = [0,1,2,...N!0-1] // or min_index!3 = [i, (i:N!0)]

// i is redundant for min_sum!2, min_sum!4 but not for !12![i]!
for i in range(0, N!0):
  min_sum!2 = PHI(min_sum!1, min_sum!4) 
  !1!2[i] = (sum!3[i] < min_sum!2)
  min_sum!4 = MUX(!1!2[i], sum!3[i], min_sum!2)

// same, i is redundant for min_index!2, min_index!4
for i in range(0, N!0):
  min_index!2 = PHI(min_index!1, min_index!4)  
   min_index!4 = MUX(!1!2[i], min_index!3[i], min_index!2)

// drop_dim becomes redundant
!2!1 = (min_sum!2, min_index!2)
\end{verbatim}
}

\subsubsection{Phase 4 of Basic Vectorization}

And this phase adds SIMD operations:

{\small
\begin{verbatim}
min_sum!1 = 10000
min_index!1 = 0
S!0^ = raise_dim(S!0, ((i * D!0) + j), (i:N!0,j:D!0))
C!0^ = raise_dim(C!0, j, (i:N!0,j:D!0))

sum!2 = [0,..,0]
d!3[I,J] = SUB_SIMD(S!0^[I,J],C!0^[I,J])
p!3[I,J] = MUL_SIMD(d!3[I,J] * d!3[I,J])

for j in range(0, D!0):
  // I dim is a noop. sum is already a one-dimensional vector
  sum!3[I] = PHI(sum!2^[I], sum!4[I])       
  sum!4[I] = ADD_SIMD(sum!3[I],p!3[I,j])

min_index!3 = [0,1,...N!0-1]   

for i in range(0, N!0):
  min_sum!2 = PHI(min_sum!1, min_sum!4) 
  !1!2[i] = CMP(sum!3[i],min_sum!2)
  min_sum!4 = MUX(!1!2[i], sum!3[i], min_sum!2)
    
for i in range(0, N!0):
  min_index!2 = PHI(min_index!1, min_index!4)  
  min_index!4 = MUX(!1!2[i], min_index!3[i], min_index!2)
   
!2!1 = (min_sum!2, min_index!2)   
\end{verbatim}
}


\ana{TODO: It will be good to add a pass in Phase 3, or post Phase 3, to get rid of extra dimensionality. 
Once we have vectorized as much as possible, there is no need to keep higher 
dimensionality. E.g., there is no need for the 2-dimensional array sum in the first for loop, 
and similarly, there is no need for 1-dimensional arrays for min\_sum and min\_index.
We should get rid of these before generating MOTION code.}

\ana{I don't have a crisp algorithm in mind. I don't think it will be difficult but we have to 
think this through. Put on our TODO list for us to think about since it will simplify MOTION 
code tremendously.}

\ana{Begin Work-in-progress notes on implementation. Just some thoughts...} 

For now, I suggest we go with a "naive" implementation and try to do only minimal optimizations. 
All arrays are stored in the program as one-dimensional arrays. Arrays lifted from scalars have dimensionality of the loop enclosure in 
the original code, e.g., \texttt{sum!3} is two dimensional \texttt{[N!0,D!0]} (but represented linearly in row-major order; all these "copies" are 
arrays of references to shares). Naturally, all writes into these arrays are canonical writes, i.e., $A[i,j] = ...$ and no things like $A[i+1,j-1] = ...$.

The loop "for j in range..." is perhaps the most interesting. First, consider "projection", e.g., p!3[I,j]. 
We fix j (the current iteration) and iterate over I using the row-major formula; it returns the j-th column, 
a one-dimensional column vector. I think this can be generalized across any mix of vectorized vs. 
non-vectorized dimensions but I'm not 100\% sure.

\begin{verbatim}
// project the (j-1)-st column vector:
A = sum!4[I,j-1]
// project j-th column from p!3: 
B = p!3[I,j] 
// we get a N!0-size vector of references to NEW shares:
C = ADD_SIMD(A,B) 
// Writes j-th column of sum!4 with the new references:
sum!4[I,j] = C 
// may eventually optimize since C is A
\end{verbatim}

There are three kinds of arrays (for now all kept internally as one-dimensional arrays, but that's under discussion).
\begin{itemize}

\item Scalars: These are scalar variables we lift into arrays for the purposes of vectorization. 
For those, all writes are canonical writes and all reads are canonical reads. We may apply both raise dimension 
and drop dimension on these.

\item Read-only input arrays: Read-only inputs. There are NO writes, while we may have non-canonical reads, $f(i,j,k)$. 
Phase 1 of Basic vectorization will add raise dimension operation at the beginning of the function and raise dimension 
may reshape arrays. If there are multiple "views" of the input array, there could be multiple raise dimension statements to create
each one of these views. The invariant is that at reads in loops, the reads of "views" of the original input array will be canonical.
Only raise dimension applies.
 
 \item Read-write output arrays: Writes are canonical (by restriction) but reads can be non-canonical. Conjecture:
 do nothing. Dependence analysis takes care of limiting vectorization so non-canonical access will work. 
Still work in progress. We may apply both raise and drop dimension. 

\end{itemize}

\ana{End Work-in-progress notes.}

%\end{document}
