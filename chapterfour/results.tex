\section{Experimental Results}
\label{sec:results}

\input{table1.tex}



We compared 49 benchmarks, 23 of which are loops taken from
the test code in HyCC \cite{Buscher2018}, while the other 26 are part of a micro benchmark suite cases 
we developed for this purpose. \tabref{table:resultstablehycc} shows our results from the HyCC loops 
and \tabref{table:resultstablemicro} shows our results on our own benchmark suite. We found that
our assumptions held for all the test cases. 
The majority of our programs benefited from our analysis with $\nicefrac{40}{49}$ programs showing
a run time improvement. 

\input{table2.tex}

As stated at the end of \secref{sec:loopscheduling} most of the loops we analyzed from HyCC were manipulations
of vectorized multi-dimensional arrays into output arrays or values. Since the multi-dimensional arrays
were represented in single dimensional format, indexes on these arrays were almost always very simple operations.
\figref{fig:MNIST2HyCC} shows a typical example of this; the arrays {\sf kernel} and {\sf image} are used to modify the 
loop output variable {\sf tmp}. The only across-loop dependency
is {\sf tmp} on itself which can be vectorized much like the inner-product example in 
\secref{sec:mpcamortization} using divide-and-conquer. In many cases we also found that 
as parallelization of independent operations increased, the cost most often 
decreased from $O(n)$ to either $O(log_{2}(n))$ or $O(1)$. The results
for this are  shown in \tabref{table:runtimeresults}. Not all problems showed changed;
For example, simple assignments incur no cost meaning that any loop body that consists of 
\emph{only} assignments shows no improvement. This is the case for DB_JOIN_TWO1 (shown in \figref{fig:DBJOINTWO1HyCC}).
In general our hand crafted benchmarks were significantly more difficult than those of HyCC. 
Consider Test6 (shown in \figref{fig:Example6}). The use of many different
conditional statements and complex dependencies lead to the high vectorized cost. 
Whereas in the HyCC benchmarks we found that most dependencies were quite simple.
An important note is that none of the the number of sequential opereations in our tests 
ever increased, they only decreased or remained costant. 
This means that existing techniques for loop optimizations only benefit from our work. 

Any schedule of SCC components that consists of one SCC node of any complexity flowing into a fully 
parallelizable node (performing $n$ operations simultaneously) is optimal when executed linearly. We found this
to be the case in the majority of the HyCC loop bodies. All of the benchmarks in the HyCC suite were found to be optimal.
On the other hand any, as we stated in \secref{sec:theoreticalguarentees}, any SCC DAG that contains edges 
that connect a non constant rate node to another non-constant rate node node is not known to be optimal. 
Again, this is because our analysis does not provide a way to optimally schedule the statements within the 
individual iterations in parallel. Node pairs of this type have unique dependencies, and running them linearly may miss 
places where statements from one node can be run in parallel with statements from the other
node. We did find that the majority of our tests were indeed optimal, this is shown in
\tabref{table:optimalclassification}. This interleaving of statements between SCC nodes is most certainly an area 
for future work, and possible proofs for optimality. Despite a lack of a formal optimality result we saw
that almost all of our test cases showed a drastic improvement in run time complexity.

\input{table3.tex}

\begin{figure}[h]
\centering
\begin{minipage}{0.70\textwidth}
\begin{javacode}
...
for(wx = 0; wx < window_size; wx++) {
    int convPos = wx+wy*window_size;
    tmp += kernel[convPos] 
    	* image[(y*stride + wy) 
    	* image_width 
    	+ (x*stride + wx)];
}
...
\end{javacode}
\end{minipage}
\caption{Example taken from MNIST2 in HyCC. %\ana{This is O(logN), no? Just a readonly array and a sum.}
%\lindsey{yes! I reference it in the second paragraph of \secref{sec:results} to illustrate the fact that most loops are manipulating vectorized multi-dimensional arrays}
}
\label{fig:MNIST2HyCC}
\end{figure}

\begin{figure}[h]
\centering
\begin{minipage}{0.70\textwidth}
\begin{javacode}
...
for(int j = from; j < n; j++) {
    int tmp = m[from*n+j];
    m[from*n+j] = m[to*n+j];
    m[to*n+j] = tmp;
}
...
\end{javacode}
\end{minipage}
\caption{Example taken from DB_JOIN_TWO in HyCC. %\ana{This is just 0, no? It's local computation since it's just}
%\lindsey{referenced above to show that not all programs change since this is just a cost of 0}
}
\label{fig:DBJOINTWO1HyCC}
\end{figure}


\begin{figure}[h]
\centering
\begin{minipage}{0.70\textwidth}
\begin{javacode}
...
k = i;
z = z + i;
a[i] = i;
a[k] = i + a[i];
int j = i - 1;
if(a[i] > 3) {
    a[k] = p * 10;
    if(p > 20) {
        a[i] = p * 20;
    } else {
        a[k] = p * 30;
    }
    z = z + a[i];
} else {
    a[i] = p * 40;
}
q = q + a[i];
i++;
...
\end{javacode}
\end{minipage}
\caption{Test6 from our handcrafted benchmarks.}
\label{fig:Example6}
\end{figure}
